{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuriborg/dsmkt/blob/main/testes_modelos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas"
      ],
      "metadata": {
        "id": "xNY-S-HJWAS-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RggFS4TBVzva"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import os, sys\n",
        "import time\n",
        "import datetime\n",
        "import warnings\n",
        "import pickle\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "dt = datetime.datetime.now().strftime('%d%m%Y_%H%M%S')\n",
        "\n",
        "#import missingno as msno\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, recall_score, precision_score, f1_score, fbeta_score\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score\n",
        "import statistics\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from imbalanced_ensemble.metrics import classification_report_imbalanced\n",
        "\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_with_sampler\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from imblearn import FunctionSampler\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvar plots no Bucket"
      ],
      "metadata": {
        "id": "vIwlfYjbWFvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_bucket_obj(sub_str):\n",
        "    s3 = boto3.resource(\n",
        "        service_name='s3',\n",
        "        region_name='us-east-1',\n",
        "        aws_access_key_id= access_key,\n",
        "        aws_secret_access_key= secret_access_key\n",
        "    )\n",
        "    for obj in s3.Bucket(\"sami-data-platform-s3-dev-sandbox-data-science\").objects.all():\n",
        "        if sub_str in obj._key:\n",
        "            print(obj)\n",
        "\n",
        "\n",
        "def save_html_plot_in_bucket(\n",
        "    filename,\n",
        "    my_bucket = \"sami-data-platform-s3-dev-sandbox-data-science\",\n",
        "    path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com/plots_churn'\n",
        "):\n",
        "    s3 = boto3.resource(\n",
        "        service_name='s3',\n",
        "        region_name='us-east-1',\n",
        "        aws_access_key_id=access_key,\n",
        "        aws_secret_access_key=secret_access_key\n",
        "    )\n",
        "    input_file = os.path.join(\n",
        "        path,\n",
        "        filename\n",
        "    )\n",
        "    s3.Bucket(my_bucket).upload_file(input_file, filename)"
      ],
      "metadata": {
        "id": "vl8hsIudWGEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_csv_from_bucket('churn_preprocessed_21112022.csv')\n",
        "2\n",
        "labels = ['contract_id', 'e_desligado']\n",
        "3\n",
        "features = list(set(df.columns) - set(labels))\n",
        "4\n",
        "df = df[labels + features]\n",
        "5\n",
        "df"
      ],
      "metadata": {
        "id": "kmoB0gN3WNum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifiers"
      ],
      "metadata": {
        "id": "Z4bYkAOQWPFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import lightgbm as lgb\n",
        "\n",
        "def get_classifiers(\n",
        "    RANDOM_STATE = 42\n",
        "):\n",
        "\n",
        "    classifiers = {\n",
        "        'model': [\n",
        "                    # 'BalancedBagging+RandomOverSampler',\n",
        "                    # 'Bagging+DecisionTree',\n",
        "                    # 'BaggingClassifier',\n",
        "                    # 'BalancedBagging+SMOTE',\n",
        "                    # 'BalancedBagging+roughly_balanced_bagging',\n",
        "\n",
        "                    'Balanced bag of histogram gradient boosting',\n",
        "\n",
        "                    # 'HistGradientBoostingClassifier',\n",
        "                    'Balanced RandomForest',\n",
        "                    # 'BalancedBagging+RandomUnderSample',\n",
        "                    'Undersampling RandomForest',\n",
        "                    'Undersampling LogisticRegression',\n",
        "                    # 'RandomForest+BalancedClassWeights',\n",
        "                    'Undersampling XGBoost',\n",
        "                    'Oversampling XGBoost',\n",
        "                    # 'XGBoost',\n",
        "                    'Bagging+KNN10',\n",
        "                    # 'Bagging LinearSVC',\n",
        "                    # 'LinearSVC'\n",
        "\n",
        "                    'LightGBM',\n",
        "                    'CatBoost'\n",
        "        ],\n",
        "        'classifier':[\n",
        "                    # BalancedBaggingClassifier(\n",
        "                    #                             sampler=RandomOverSampler(),\n",
        "                    #                             n_estimators=100),\n",
        "                    # BaggingClassifier(\n",
        "                    #                     base_estimator=DecisionTreeClassifier(),\n",
        "                    #                     random_state=RANDOM_STATE,\n",
        "                    #                     n_estimators=100),\n",
        "                    # BaggingClassifier(n_estimators=100),\n",
        "                    # BalancedBaggingClassifier(sampler=SMOTE(),\n",
        "                    #                             n_estimators=100),\n",
        "                    # BalancedBaggingClassifier(\n",
        "                    #     sampler=FunctionSampler(func=roughly_balanced_bagging, kw_args={\"replace\": True}),\n",
        "                    #     n_estimators=25\n",
        "                    # ),\n",
        "\n",
        "                    make_pipeline(\n",
        "                        BalancedBaggingClassifier(\n",
        "                            base_estimator=HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
        "                            n_estimators=10,\n",
        "                            random_state=RANDOM_STATE,\n",
        "                            n_jobs=2,\n",
        "                        ),\n",
        "                    ),\n",
        "\n",
        "                    # HistGradientBoostingClassifier(random_state=RANDOM_STATE,\n",
        "                    #                                 learning_rate=0.9),\n",
        "                    make_pipeline(\n",
        "                      BalancedRandomForestClassifier(random_state=RANDOM_STATE,\n",
        "                                                     n_jobs=2,\n",
        "                                                    n_estimators=50),\n",
        "                  ),\n",
        "                    # BalancedBaggingClassifier(sampler=RandomUnderSampler(),\n",
        "                    #                             n_estimators=25),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomUnderSampler(random_state=RANDOM_STATE),\n",
        "                        RandomForestClassifier(random_state=RANDOM_STATE,\n",
        "                                                n_jobs=2),\n",
        "                    ),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomUnderSampler(random_state=RANDOM_STATE),\n",
        "                        LogisticRegression(max_iter=1000),\n",
        "                    ),\n",
        "                    # RandomForestClassifier(random_state=RANDOM_STATE,\n",
        "                    #                         class_weight='balanced',\n",
        "                    #                         n_jobs=2,\n",
        "                    #                         n_estimators=50),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomUnderSampler(random_state=RANDOM_STATE),\n",
        "                        xgb.XGBClassifier(),\n",
        "                    ),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomOverSampler(random_state=RANDOM_STATE),\n",
        "                        xgb.XGBClassifier(),\n",
        "                    ),\n",
        "                    # xgb.XGBClassifier(scale_pos_weight=100),\n",
        "                    BaggingClassifier(\n",
        "                                        base_estimator=KNeighborsClassifier(n_neighbors=10),\n",
        "                                        random_state=RANDOM_STATE\n",
        "                                        ),\n",
        "                    # BaggingClassifier(\n",
        "                    #                     base_estimator=svm.SVC(kernel='linear',\n",
        "                    #                                             C=1,\n",
        "                    #                                             random_state=RANDOM_STATE,\n",
        "                    #                                             probability = True),\n",
        "                    #                     random_state=RANDOM_STATE\n",
        "                    #                     ),\n",
        "                    # svm.SVC(\n",
        "                    #     kernel='linear',\n",
        "                    #     random_state = RANDOM_STATE,\n",
        "                    #     class_weight='balanced',\n",
        "                    #     gamma='scale',\n",
        "                    #     C=1.1,\n",
        "                    #    probability=True\n",
        "                    # ),\n",
        "                    LGBMClassifier(random_state = RANDOM_STATE),\n",
        "                    CatBoostClassifier(random_state = RANDOM_STATE)\n",
        "        ],\n",
        "        'randsearch_params': [\n",
        "                        # # 'BalancedBagging+RandomOverSampler',\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # 'Bagging+DecisionTree'\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # BaggingClassifier\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # BalancedBagging+SMOTE\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # BalancedBagging+roughly_balanced_bagging\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "\n",
        "                        # 'Balanced bag of histogram gradient boosting'\n",
        "                        {'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "\n",
        "                        # # HistGradientBoostingClassifier\n",
        "                        # {'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "                        # BalancedRandomForestClassifier\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # # 'BalancedBagging+RandomUnderSample'\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # 'Undersampling RandomForest'\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # 'Undersampling LogisticRegression'\n",
        "                        {'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "                        'tol': [1e-4, 1e-3],\n",
        "                        'C': [1.0, 0.9, 1.1]},\n",
        "\n",
        "                        # # 'RandomForest+BalancedClassWeights'\n",
        "                        # {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # 'Undersampling XGBoost'\n",
        "                        {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                        # 'Oversampling XGBoost'\n",
        "                        {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                        # # XGBoost\n",
        "                        # {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                        # 'Bagging+KNN10'\n",
        "                        {'weights': ['uniform', 'distance'],\n",
        "                        'n_neighbors': [3, 5, 8, 10, 15]},\n",
        "\n",
        "                        # # # Bagging+LinearSVC\n",
        "                        # # {'gamma': ['scale', 'auto'],\n",
        "                        # # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                        # # LinearSVC\n",
        "                        # {'gamma': ['scale', 'auto'],\n",
        "                        # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                        # LightGBM params\n",
        "                        {\n",
        "                            'max_depth':[-1, 50, 100, 200],\n",
        "                            'learning_rate': [0.001, 0.01, 0.1, 0.5, 0.9, 1.0, 1.2, 1.5],\n",
        "                            'n_estimators': [100, 150, 200, 250, 500],\n",
        "#                             'class_weight': [None, {0: 0.9, 1: 0.1}],\n",
        "                            'scale_pos_weight':[None, 1.0, 0.1, 0.05]\n",
        "                        },\n",
        "\n",
        "                        # CatBoost\n",
        "                        {\n",
        "                            'max_depth':[-1, 50, 100, 200],\n",
        "                            'learning_rate':[0.001, 0.01, 0.1, 0.5, 0.9, 1.0, 1.2, 1.5],\n",
        "                            'n_estimators': [100, 150, 200, 250, 500],\n",
        "                            'scale_pos_weight':[None, 1.0, 0.1, 0.05]\n",
        "                        },\n",
        "        ],\n",
        "        'gridsearch_params': [\n",
        "                        # # 'BalancedBagging+RandomOverSampler',\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'Bagging+DecisionTree'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'BaggingClassifier'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'BalancedBagging+SMOTE'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'BalancedBagging+roughly_balanced_bagging'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "\n",
        "                        # 'Balanced bag of histogram gradient boosting'\n",
        "                        {'loss': ['auto']*5,\n",
        "                            'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "\n",
        "                        # # HistGradientBoostingClassifier\n",
        "                        # {'loss': ['auto']*5,\n",
        "                        #     'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "                        # BalancedRandomForestClassifier\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # # 'BalancedBagging+RandomUnderSample'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # 'Undersampling RandomForest'\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # 'Undersampling LogisticRegression'\n",
        "                        {'penalty': ['l1', 'l2', 'elasticnet']*3,\n",
        "                        'C': [1.0, 1.0, 1.0,\n",
        "                                0.9, 0.9, 0.9,\n",
        "                                1.1, 1.1, 1.1]},\n",
        "\n",
        "                        #     # 'RandomForest+BalancedClassWeights'\n",
        "                        #     {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                            # 'Undersampling XGBoost'\n",
        "                            {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                            # 'Oversampling XGBoost'\n",
        "                            {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                            # # XGBoost\n",
        "                            # {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                            # 'Bagging+KNN10'\n",
        "                            {'weights': ['uniform']*5 + ['distance']*5,\n",
        "                            'n_neighbors': [3, 5, 8, 10, 15]*2},\n",
        "\n",
        "                            # # # Bagging+LinearSVC\n",
        "                            # # {'gamma': ['scale']*4 + ['auto']*4,\n",
        "                            # # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                            # # LinearSVC\n",
        "                            # {'gamma': ['scale']*4 + ['auto']*4,\n",
        "                            # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                            # LGBM\n",
        "                            {},\n",
        "\n",
        "                            # CatBoost\n",
        "                            {}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "    # for k in list(classifiers.keys()):\n",
        "    #   print(f'key: {k}, len: {len(classifiers[k])}')\n",
        "\n",
        "    return pd.DataFrame(classifiers)\n",
        "\n",
        "df_classifiers = get_classifiers()\n",
        "df_classifiers"
      ],
      "metadata": {
        "id": "zSCVcJVuWPWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função de Teste de Modelos"
      ],
      "metadata": {
        "id": "minasvvCWYkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode('utf-8')\n",
        "\n",
        "\n",
        "def filter_alphanum_str(x):\n",
        "    return remove_accents(re.sub(r'\\W+', '', x))\n",
        "\n",
        "def get_features_labels_subset(df,\n",
        "                               features,\n",
        "                               p = 0.8,\n",
        "                               target_label = 'e_desligado',\n",
        "                               adjust_columns = False\n",
        "                              ):\n",
        "    t = df.copy()\n",
        "\n",
        "    if adjust_columns:\n",
        "        t.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   t.columns))\n",
        "        t.columns = list(map(filter_alphanum_str, t.columns))\n",
        "\n",
        "        features = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   features))\n",
        "        features = list(map(filter_alphanum_str, features))\n",
        "\n",
        "\n",
        "    N = len(t)\n",
        "    N_train = round(N * p)\n",
        "    N_test = N - N_train\n",
        "\n",
        "    X_train, X_test = t[features].loc[:N_train], t[features].loc[N_train:]\n",
        "    y_train, y_test = t[target_label].loc[:N_train].values, t[target_label].loc[N_train:].values\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf,\n",
        "    model_name,\n",
        "    adjust_X_train_columns = False\n",
        "):\n",
        "    if adjust_X_train_columns:\n",
        "        X_train.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-',''),\n",
        "                                   X_train.columns))\n",
        "        X_test.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-',''),\n",
        "                                   X_test.columns))\n",
        "\n",
        "        X_train.columns = list(map(filter_alphanum_str, X_train.columns))\n",
        "        X_test.columns = list(map(filter_alphanum_str, X_test.columns))\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_pred_proba = clf.predict_proba(X_test)\n",
        "\n",
        "    report = pd.DataFrame(classification_report_imbalanced(y_test, y_pred, output_dict=True))\n",
        "\n",
        "    d = {\n",
        "        'metric': list(report[1].index[:5]),\n",
        "        'values': list(map(lambda x: round(x, 3), report[1].values[:5])),\n",
        "        'model': len(report[1].values[:5])*[model_name]\n",
        "    }\n",
        "    d['metric'].append('accuracy')\n",
        "    d['values'].append(round(accuracy_score(y_true = y_test,\n",
        "                                     y_pred = y_pred),\n",
        "                             3)\n",
        "                      )\n",
        "    d['model'].append(model_name)\n",
        "    report_churn = pd.DataFrame(d)\n",
        "\n",
        "    return report_churn"
      ],
      "metadata": {
        "id": "BukaM9OpWVoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_features_labels_balanced(\n",
        "    df,\n",
        "    features,\n",
        "    p = 0.8,\n",
        "    target_label = 'e_desligado',\n",
        "   adjust_columns = False\n",
        "):\n",
        "    t = df.copy()\n",
        "\n",
        "    # shuffling indexes\n",
        "    t = t.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    features = list(set(features) & set(t.columns))\n",
        "\n",
        "    if adjust_columns:\n",
        "        t.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   t.columns))\n",
        "        t.columns = list(map(filter_alphanum_str, t.columns))\n",
        "\n",
        "        features = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   features))\n",
        "        features = list(map(filter_alphanum_str, features))\n",
        "\n",
        "    # train data qtd\n",
        "    N = len(t)\n",
        "    N_train = math.ceil(N * p)\n",
        "    N_test = N - N_train\n",
        "\n",
        "    # initial balancement\n",
        "    qtd_pos = len(t[t[target_label] == 1])\n",
        "    qtd_neg = len(t[t[target_label] == 0])\n",
        "    print('qtd_pos: {} ({:.3f}%), qtd_neg: {} ({:.3f}%)'.format(\n",
        "        qtd_pos, (qtd_pos*100/(qtd_pos+qtd_neg)),\n",
        "        qtd_neg, (qtd_neg*100/(qtd_pos+qtd_neg))\n",
        "    ))\n",
        "    # label proportion\n",
        "    N_train_pos = math.ceil(qtd_pos * p)\n",
        "    N_train_neg =  math.ceil(qtd_neg * p)\n",
        "    N_tot = N_train_pos + N_train_neg\n",
        "    print('N_train_pos: {} ({:.3f}%), N_train_neg: {} ({:.3f}%)'.format(\n",
        "        N_train_pos, (N_train_pos*100/N_tot),\n",
        "        N_train_neg, (N_train_neg*100/N_tot)\n",
        "    ))\n",
        "    # positive samples\n",
        "    pos_train_data_sample = t[t[target_label] == 1].reset_index(drop=True).loc[:N_train_pos]\n",
        "\n",
        "    X_train_pos = pos_train_data_sample[features]\n",
        "    y_train_pos = pos_train_data_sample[target_label].values\n",
        "\n",
        "    # negative samples\n",
        "    neg_train_data_sample = t[t[target_label] == 0].reset_index(drop=True).loc[:N_train_neg]\n",
        "\n",
        "    X_train_neg = neg_train_data_sample[features]\n",
        "    y_train_neg = neg_train_data_sample[target_label].values\n",
        "\n",
        "\n",
        "    # Training data\n",
        "    X_train = pd.concat([X_train_pos, X_train_neg],\n",
        "                       axis = 0)\n",
        "    y_train = list(y_train_pos) + list(y_train_neg)\n",
        "\n",
        "\n",
        "    # test data\n",
        "    test_data_sample = t[\n",
        "        (~t.index.isin(pos_train_data_sample.index)) &\n",
        "        (~t.index.isin(neg_train_data_sample.index))\n",
        "    ]\n",
        "    X_test = test_data_sample[features]\n",
        "    y_test = test_data_sample[target_label].values\n",
        "\n",
        "    return X_train, np.array(y_train), X_test, np.array(y_test)\n"
      ],
      "metadata": {
        "id": "ebZ_C_2hWYNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balanceamento dos dados"
      ],
      "metadata": {
        "id": "_OkC8K2jWc7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                           features,\n",
        "                                           p = 0.8,\n",
        "                                          adjust_columns = True\n",
        "                                   )\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "id": "IDIbeKoAWdYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balanceamento dos dados"
      ],
      "metadata": {
        "id": "8Pzlgj1CWkAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(set(df.columns) - set(labels))\n",
        "\n",
        "feats_remove = [\n",
        "    'churn_não churnou',\n",
        "    'churn_churnou',\n",
        "#     'tempo_churn_meses',\n",
        "    'prestador_mais_freq_nan',\n",
        "    'tipo_contato_nps_Membro - Cancelado',\n",
        "    'tipo_contato_nps_Contratante - Não membro'\n",
        "]\n",
        "features = list(set(features) - set(feats_remove))\n",
        "features = list(filter(lambda x: 'não preenchido' not in x and x[-4:] != '_na',\n",
        "                      features\n",
        "                      ))\n",
        "X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                           features,\n",
        "                                           p = 0.8,\n",
        "                                          adjust_columns = True\n",
        "                                   )\n",
        "report_dummy_clf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = DummyClassifier(),\n",
        "    model_name = 'Dummy Classifier'\n",
        ")\n",
        "report_log_reg = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = LogisticRegression(),\n",
        "    model_name = 'Logistic Reg'\n",
        ")\n",
        "report_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = RandomForestClassifier(random_state = 42),\n",
        "    model_name = 'Random Forest'\n",
        ")\n",
        "report_us_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling RandomForest']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp RandomForest'\n",
        ")\n",
        "report_brf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Balanced RandomForest']['classifier'].values[0],\n",
        "    model_name = 'Balanced RandomForest'\n",
        ")\n",
        "report_us_xb = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling XGBoost']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp XGBoost'\n",
        ")\n",
        "report_lgbm = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'LightGBM']['classifier'].values[0],\n",
        "    model_name = 'LightGBM'\n",
        ")\n",
        "report_catboost = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'CatBoost']['classifier'].values[0],\n",
        "    model_name = 'CatBoost'\n",
        ")\n",
        "\n",
        "reports = pd.concat([\n",
        "                        report_dummy_clf,\n",
        "                        report_log_reg,\n",
        "                        report_rf,\n",
        "                        report_us_rf,\n",
        "                        report_brf,\n",
        "                        report_us_xb,\n",
        "                        report_lgbm,\n",
        "                        report_catboost\n",
        "                    ], axis = 0)\n",
        "\n",
        "fig = px.line_polar(reports,\n",
        "                        r=\"values\",\n",
        "                        theta='metric',\n",
        "                       color = 'model',\n",
        "                        line_close=True,\n",
        "                        height = 400,\n",
        "                        width = 700\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features)),\n",
        "    title_x = 0.5,\n",
        "#         title_y = 0.6,\n",
        "    xaxis_title=\"variáveis\",\n",
        "    yaxis_title=\"minmax score\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=12,\n",
        "    ),\n",
        "    legend = dict(\n",
        "        title = 'Modelos iniciais'\n",
        "    )\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(reports,\n",
        "                     y=\"values\",\n",
        "                    x='metric',\n",
        "                    color = 'model',\n",
        "                    barmode = 'group',\n",
        "                    text='values',\n",
        "                    height = 400,\n",
        "                    width = 1100\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features)),\n",
        "    title_x = 0.5,\n",
        "    yaxis_title=\"Performance\",\n",
        "    xaxis_title=\"Métrica\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=16,\n",
        "    ),\n",
        ")\n",
        "out_path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com/plots_churn'\n",
        "titulo = 'dataleakage'\n",
        "out_file = os.path.join(out_path,\n",
        "                       'clf_results_{}_{}.html'.format(\n",
        "                            titulo,\n",
        "                           dt\n",
        "                            )\n",
        "                       )\n",
        "fig.write_html(out_file)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "wumh9TqVWlNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(filter(lambda x: 'clf_results_dataleakage' in x,\n",
        "           os.listdir(out_path)\n",
        "           ))"
      ],
      "metadata": {
        "id": "H6pNIXtfWkQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list(filter(lambda x: 'dataleakage_191022022' in x, os.listdir(out_path)))\n",
        "save_html_plot_in_bucket(filename = f'clf_results_dataleakage_{dt}.html')\n",
        "find_bucket_obj(f'clf_results_dataleakage_{dt}')"
      ],
      "metadata": {
        "id": "xbLvBVm6WkmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_classifiers[df_classifiers.model == 'LightGBM']"
      ],
      "metadata": {
        "id": "V9n2IODKWkj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(filter(lambda x: 'churn' in x or 'tempo' in x , features))"
      ],
      "metadata": {
        "id": "7axOOXjkWuq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(set(df.columns) - set(labels))\n",
        "\n",
        "feats_remove = [\n",
        "    'churn_não churnou',\n",
        "    'churn_churnou',\n",
        "    'tempo_churn_meses',\n",
        "    'prestador_mais_freq_nan',\n",
        "    'tipo_contato_nps_Membro - Cancelado',\n",
        "    'tipo_contato_nps_Contratante - Não membro'\n",
        "]\n",
        "features = list(set(features) - set(feats_remove))\n",
        "features = list(filter(lambda x: 'não preenchido' not in x and x[-4:] != '_na',\n",
        "                      features\n",
        "                      ))\n",
        "X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                           features,\n",
        "                                           p = 0.8,\n",
        "                                          adjust_columns = True\n",
        "                                   )\n",
        "\n",
        "report_dummy_clf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = DummyClassifier(),\n",
        "    model_name = 'Dummy Classifier'\n",
        ")\n",
        "report_log_reg = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = LogisticRegression(),\n",
        "    model_name = 'Logistic Reg'\n",
        ")\n",
        "report_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = RandomForestClassifier(random_state = 42),\n",
        "    model_name = 'Random Forest'\n",
        ")\n",
        "report_us_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling RandomForest']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp RandomForest'\n",
        ")\n",
        "report_brf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Balanced RandomForest']['classifier'].values[0],\n",
        "    model_name = 'Balanced RandomForest'\n",
        ")\n",
        "report_us_xb = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling XGBoost']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp XGBoost'\n",
        ")\n",
        "report_lgbm = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'LightGBM']['classifier'].values[0],\n",
        "    model_name = 'LightGBM'\n",
        ")\n",
        "report_catboost = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'CatBoost']['classifier'].values[0],\n",
        "    model_name = 'CatBoost'\n",
        ")\n",
        "\n",
        "reports = pd.concat([\n",
        "                        report_dummy_clf,\n",
        "                        report_log_reg,\n",
        "                        report_rf,\n",
        "                        report_us_rf,\n",
        "                        report_brf,\n",
        "                        report_us_xb,\n",
        "                        report_lgbm,\n",
        "                        report_catboost\n",
        "                    ], axis = 0)\n",
        "\n",
        "fig = px.line_polar(reports,\n",
        "                        r=\"values\",\n",
        "                        theta='metric',\n",
        "                       color = 'model',\n",
        "                        line_close=True,\n",
        "                        height = 400,\n",
        "                        width = 700\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features)),\n",
        "    title_x = 0.5,\n",
        "#         title_y = 0.6,\n",
        "    xaxis_title=\"variáveis\",\n",
        "    yaxis_title=\"minmax score\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=12,\n",
        "    ),\n",
        "    legend = dict(\n",
        "        title = 'Modelos iniciais'\n",
        "    )\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(reports,\n",
        "                     y=\"values\",\n",
        "                    x='metric',\n",
        "                    color = 'model',\n",
        "                    barmode = 'group',\n",
        "                    text='values',\n",
        "                    height = 400,\n",
        "                    width = 1100\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features)),\n",
        "    title_x = 0.5,\n",
        "    yaxis_title=\"Performance\",\n",
        "    xaxis_title=\"Métrica\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=16,\n",
        "    ),\n",
        ")\n",
        "out_path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com/plots_churn'\n",
        "titulo = 'remocao_dataleakage'\n",
        "out_file = os.path.join(out_path,\n",
        "                       'clf_results_{}_{}.html'.format(\n",
        "                            titulo,\n",
        "                           dt\n",
        "                            )\n",
        "                       )\n",
        "fig.write_html(out_file)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "cLVzVaxaWuoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list(filter(lambda x: 'remocao_dataleakage_19102022' in x, os.listdir(out_path)))\n",
        "save_html_plot_in_bucket(filename = f'clf_results_remocao_dataleakage_{dt}.html')\n",
        "find_bucket_obj(f'clf_results_remocao_dataleakage_{dt}')"
      ],
      "metadata": {
        "id": "AyrVzrzkWuk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste - PCA + Todas as features"
      ],
      "metadata": {
        "id": "2sv5HVBXW19n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_pca = 20\n",
        "\n",
        "features = list(set(df.columns) - set(labels))\n",
        "\n",
        "feats_remove = [\n",
        "    'churn_não churnou',\n",
        "#     'churn_churnou',\n",
        "#     'tempo_churn_meses',\n",
        "    'prestador_mais_freq_nan',\n",
        "    'tipo_contato_nps_Membro - Cancelado',\n",
        "    'tipo_contato_nps_Contratante - Não membro'\n",
        "]\n",
        "features = list(set(features) - set(feats_remove))\n",
        "features = list(filter(lambda x: 'não preenchido' not in x and x[-4:] != '_na',\n",
        "                      features\n",
        "                      ))\n",
        "X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                           features,\n",
        "                                           p = 0.8,\n",
        "                                          adjust_columns = True\n",
        "                                   )\n",
        "pca = PCA(n_components=n_pca)\n",
        "pca.fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "\n",
        "pca = PCA(n_components=n_pca)\n",
        "pca.fit(X_test)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "\n",
        "report_dummy_clf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = DummyClassifier(),\n",
        "    model_name = 'Dummy Classifier',\n",
        "    adjust_X_train_columns = False\n",
        ")\n",
        "report_log_reg = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = LogisticRegression(),\n",
        "    model_name = 'Logistic Reg',\n",
        "    adjust_X_train_columns = False\n",
        ")\n",
        "report_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = RandomForestClassifier(random_state = 42),\n",
        "    model_name = 'Random Forest',\n",
        "    adjust_X_train_columns = False\n",
        ")\n",
        "report_us_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling RandomForest']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp RandomForest',\n",
        "    adjust_X_train_columns = False\n",
        ")\n",
        "report_brf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Balanced RandomForest']['classifier'].values[0],\n",
        "    model_name = 'Balanced RandomForest',\n",
        "    adjust_X_train_columns = False\n",
        ")\n",
        "report_us_xb = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling XGBoost']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp XGBoost',\n",
        "    adjust_X_train_columns = False\n",
        ")\n",
        "report_lgbm = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'LightGBM']['classifier'].values[0],\n",
        "    model_name = 'LightGBM'\n",
        ")\n",
        "report_catboost = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'CatBoost']['classifier'].values[0],\n",
        "    model_name = 'CatBoost'\n",
        ")\n",
        "\n",
        "reports = pd.concat([\n",
        "                        report_dummy_clf,\n",
        "                        report_log_reg,\n",
        "                        report_rf,\n",
        "                        report_us_rf,\n",
        "                        report_brf,\n",
        "                        report_us_xb,\n",
        "                        report_lgbm,\n",
        "                        report_catboost\n",
        "                    ], axis = 0)\n",
        "\n",
        "fig = px.line_polar(reports,\n",
        "                        r=\"values\",\n",
        "                        theta='metric',\n",
        "                       color = 'model',\n",
        "                        line_close=True,\n",
        "                        height = 400,\n",
        "                        width = 700\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn (PCA {} features)'.format(X_train.shape[1]),\n",
        "    title_x = 0.5,\n",
        "#         title_y = 0.6,\n",
        "    xaxis_title=\"variáveis\",\n",
        "    yaxis_title=\"minmax score\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=12,\n",
        "    ),\n",
        "    legend = dict(\n",
        "        title = 'Modelos iniciais'\n",
        "    )\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(reports,\n",
        "                     y=\"values\",\n",
        "                    x='metric',\n",
        "                    color = 'model',\n",
        "                    barmode = 'group',\n",
        "                    text='values',\n",
        "                    height = 400,\n",
        "                    width = 1100\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn (PCA {} features)'.format(X_train.shape[1]),\n",
        "    title_x = 0.5,\n",
        "    yaxis_title=\"Performance\",\n",
        "    xaxis_title=\"Métrica\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=16,\n",
        "    ),\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-fjZH9faW2cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Seleções de features\n",
        "no notebook churn_feat_selection_19102022 aplicamos múltiplos métodos de seleção de features, que podem ser mais adequados para determinados tipos de dados.\n",
        "\n",
        "Vamos considerar que os métodos de input numérico e output categórico são interessantes, visto que após scaling e encoding todas as variáveis se tornam numéricas. São eles:\n",
        "\n",
        "ANOVA (linear)\n",
        "Kendall's Tau (não linear)\n",
        "Entretanto, o teste de contingência chi2 também pode ser avaliado."
      ],
      "metadata": {
        "id": "IBLS5qKBW9Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fs_anova_prep = load_csv_from_bucket('feat_sel_ANOVA_prep_21112022.csv')\n",
        "fs_anova_num = load_csv_from_bucket('feat_sel_ANOVA_num_21112022.csv')\n",
        "\n",
        "fs_kendall_prep = load_csv_from_bucket('feat_sel_KENDALL_prep_21112022.csv')\n",
        "fs_kendall_num = load_csv_from_bucket('feat_sel_KENDALL_num_21112022.csv')\n",
        "\n",
        "fs_chi2_prep = load_csv_from_bucket('feat_sel_chi2_prep_21112022.csv')\n",
        "fs_chi2_cat = load_csv_from_bucket('feat_sel_chi2_cat_21112022.csv')"
      ],
      "metadata": {
        "id": "PBlSMd28W3Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções de teste - features"
      ],
      "metadata": {
        "id": "7SqpHzIgXDWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def performance_models_features(\n",
        "    features,\n",
        "    p =0.8,\n",
        "    title = 'Performances - previsão de churn',\n",
        "    df = df,\n",
        "    adjust_columns = True\n",
        "):\n",
        "    X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                               features,\n",
        "                                               p = p,\n",
        "                                          adjust_columns = adjust_columns\n",
        "                                       )\n",
        "\n",
        "    report_dummy_clf = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = DummyClassifier(),\n",
        "        model_name = 'Dummy Classifier'\n",
        "    )\n",
        "    report_log_reg = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = LogisticRegression(),\n",
        "        model_name = 'Logistic Reg'\n",
        "    )\n",
        "    report_rf = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = RandomForestClassifier(random_state = 42),\n",
        "        model_name = 'Random Forest'\n",
        "    )\n",
        "    report_us_rf = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'Undersampling RandomForest']['classifier'].values[0],\n",
        "        model_name = 'UnderSamp RandomForest'\n",
        "    )\n",
        "    report_brf = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'Balanced RandomForest']['classifier'].values[0],\n",
        "        model_name = 'Balanced RandomForest'\n",
        "    )\n",
        "    report_us_xb = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'Undersampling XGBoost']['classifier'].values[0],\n",
        "        model_name = 'UnderSamp XGBoost'\n",
        "    )\n",
        "    report_bbhgb = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'Balanced bag of histogram gradient boosting']['classifier'].values[0],\n",
        "        model_name = 'Balanced bag of histogram gradient boosting'\n",
        "    )\n",
        "    report_us_lr = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'Undersampling LogisticRegression']['classifier'].values[0],\n",
        "        model_name = 'Undersampling LogisticRegression'\n",
        "    )\n",
        "    report_bag_knn = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'Bagging+KNN10']['classifier'].values[0],\n",
        "        model_name = 'Bagging+KNN10'\n",
        "    )\n",
        "    report_os_xb = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'Oversampling XGBoost']['classifier'].values[0],\n",
        "        model_name = 'Oversampling XGBoost'\n",
        "    )\n",
        "    report_lgbm = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'LightGBM']['classifier'].values[0],\n",
        "        model_name = 'LightGBM'\n",
        "    )\n",
        "    report_catboost = test_model_performance(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        clf = df_classifiers[df_classifiers['model'] == 'CatBoost']['classifier'].values[0],\n",
        "        model_name = 'CatBoost'\n",
        "    )\n",
        "    reports = pd.concat([\n",
        "                            report_dummy_clf,\n",
        "                            report_log_reg,\n",
        "                            report_rf,\n",
        "                            report_us_rf,\n",
        "                            report_brf,\n",
        "                            report_us_xb,\n",
        "                            report_bag_knn,\n",
        "                            report_us_lr,\n",
        "                            report_bbhgb,\n",
        "                            report_os_xb,\n",
        "                            report_lgbm,\n",
        "                            report_catboost\n",
        "                        ], axis = 0)\n",
        "\n",
        "    fig = px.line_polar(reports,\n",
        "                            r=\"values\",\n",
        "                            theta='metric',\n",
        "                           color = 'model',\n",
        "                            line_close=True,\n",
        "                            height = 400,\n",
        "                            width = 700\n",
        "                           )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title = title,\n",
        "        title_x = 0.5,\n",
        "    #         title_y = 0.6,\n",
        "        xaxis_title=\"variáveis\",\n",
        "        yaxis_title=\"minmax score\",\n",
        "        font=dict(\n",
        "            family=\"Lato\",\n",
        "            size=12,\n",
        "        ),\n",
        "        legend = dict(\n",
        "            title = 'Modelos iniciais'\n",
        "        )\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "    fig = px.bar(reports,\n",
        "                         y=\"values\",\n",
        "                        x='metric',\n",
        "                        color = 'model',\n",
        "                        barmode = 'group',\n",
        "                        text='values',\n",
        "                        height = 400,\n",
        "                        width = 1100\n",
        "                           )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title = title,\n",
        "        title_x = 0.5,\n",
        "        yaxis_title=\"Performance\",\n",
        "        xaxis_title=\"Métrica\",\n",
        "        font=dict(\n",
        "            family=\"Lato\",\n",
        "            size=16,\n",
        "        ),\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "    return reports"
      ],
      "metadata": {
        "id": "Z7Pjwx1oW3Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtro de features"
      ],
      "metadata": {
        "id": "ht-LFqhRXI73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_features(\n",
        "    features,\n",
        "    feats_remove = None\n",
        "):\n",
        "    N_i = len(features)\n",
        "    if feats_remove is None:\n",
        "\n",
        "        feats_remove = [\n",
        "                'churn_não churnou',\n",
        "                'churn_churnou',\n",
        "                'tempo_churn_meses',\n",
        "                'prestador_mais_freq_nan',\n",
        "                'tipo_contato_nps_Membro - Cancelado',\n",
        "                'tipo_contato_nps_Contratante - Não membro'\n",
        "            ]\n",
        "    features = list(set(features) - set(feats_remove))\n",
        "    features = list(filter(lambda x: 'não preenchido' not in x and x[-4:] != '_na',\n",
        "                          features\n",
        "                          ))\n",
        "    N_remov = len(features) - N_i\n",
        "    print('{} removed, {} left'.format(N_remov, len(features)))\n",
        "    return features"
      ],
      "metadata": {
        "id": "w7_e3vZUWuiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Melhores features - ANOVA"
      ],
      "metadata": {
        "id": "DW9LYtiBXMMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## K = 20 melhores\n"
      ],
      "metadata": {
        "id": "xzkoEDXiXMhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(fs_anova_prep['Specs'].values) - set(df.columns)"
      ],
      "metadata": {
        "id": "oJQmMS9dXNNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fs_anova_prep.head(20).T"
      ],
      "metadata": {
        "id": "dItkpoEOXNL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 20\n",
        "# melhores features do anova\n",
        "features_20best_anova = list(fs_anova_prep.head(K)['Specs'].values)\n",
        "\n",
        "# removing target and unwanted\n",
        "features_20best_anova = filter_features(features_20best_anova)\n",
        "\n",
        "features_20best_anova"
      ],
      "metadata": {
        "id": "QxzjG7q8XNJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_anova_20best = performance_models_features(features_20best_anova,\n",
        "                               title = 'Performances - previsão de churn (feat.sel. 20Best ANOVA)')"
      ],
      "metadata": {
        "id": "BUKZ41n9XM0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sem tempo_churn"
      ],
      "metadata": {
        "id": "qwd-MMA4XYrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 20\n",
        "# melhores features do anova\n",
        "features_20best_anova = list(fs_anova_prep[fs_anova_prep['Specs']!= 'tempo_churn_meses'].head(K)['Specs'].values)\n",
        "\n",
        "# removing target and unwanted\n",
        "features_20best_anova = filter_features(features_20best_anova)\n",
        "\n",
        "res_anova_20best = performance_models_features(features_20best_anova,\n",
        "                               title = 'Performances - previsão de churn (feat.sel. 20Best ANOVA)')"
      ],
      "metadata": {
        "id": "_lsEEJniXY_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# socre >=0.1"
      ],
      "metadata": {
        "id": "aH9GGytIXfiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fs_anova_prep[fs_anova_prep['minmax_score'] > 0.05]"
      ],
      "metadata": {
        "id": "vpzHGHOgXf3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melhores features do anova\n",
        "features_best_anova = list(fs_anova_prep[\n",
        "        (fs_anova_prep['Specs']!= 'tempo_churn_meses') &\n",
        "        (fs_anova_prep['minmax_score'] > 0.05)\n",
        "    ]['Specs'].values)\n",
        "\n",
        "# removing target and unwanted\n",
        "features_best_anova = filter_features(features_best_anova)\n",
        "\n",
        "res_anova_best = performance_models_features(features_best_anova,\n",
        "                               title = 'Performances - previsão de churn (feat.sel. Best ANOVA)')"
      ],
      "metadata": {
        "id": "Yi4OLeCiXk2O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}