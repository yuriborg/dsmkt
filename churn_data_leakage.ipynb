{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuriborg/dsmkt/blob/main/churn_data_leakage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLJ8yMnulUUx"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import os, sys\n",
        "import time\n",
        "import datetime\n",
        "import warnings\n",
        "import pickle\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "dt = datetime.datetime.now().strftime('%d%m%Y_%H%M%S')\n",
        "\n",
        "#import missingno as msno\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, recall_score, precision_score, f1_score, fbeta_score\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score\n",
        "import statistics\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from imbalanced_ensemble.metrics import classification_report_imbalanced\n",
        "\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_with_sampler\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from imblearn import FunctionSampler\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvar Plots no Bucket\n"
      ],
      "metadata": {
        "id": "NjPUPMJ9lc4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_bucket_obj(sub_str):\n",
        "    s3 = boto3.resource(\n",
        "        service_name='s3',\n",
        "        region_name='us-east-1',\n",
        "        aws_access_key_id= access_key,\n",
        "        aws_secret_access_key= secret_access_key\n",
        "    )\n",
        "    for obj in s3.Bucket(\"sami-data-platform-s3-dev-sandbox-data-science\").objects.all():\n",
        "        if sub_str in obj._key:\n",
        "            print(obj)\n",
        "\n",
        "\n",
        "def save_html_plot_in_bucket(\n",
        "    filename,\n",
        "    my_bucket = \"sami-data-platform-s3-dev-sandbox-data-science\",\n",
        "    path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com/plots_churn'\n",
        "):\n",
        "    s3 = boto3.resource(\n",
        "        service_name='s3',\n",
        "        region_name='us-east-1',\n",
        "        aws_access_key_id=access_key,\n",
        "        aws_secret_access_key=secret_access_key\n",
        "    )\n",
        "    input_file = os.path.join(\n",
        "        path,\n",
        "        filename\n",
        "    )\n",
        "    s3.Bucket(my_bucket).upload_file(input_file, filename)"
      ],
      "metadata": {
        "id": "obxHQ2NDlZ-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS Creds\n",
        "\n"
      ],
      "metadata": {
        "id": "gCr0pYnZliML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cryptography.fernet import Fernet\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def delete_folder_files(path):\n",
        "    shutil.rmtree(path)\n",
        "def delete_file(path, filename):\n",
        "    os.remove(os.path.join(path, filename))\n",
        "\n",
        "def encode_aws_keys(\n",
        "    access_key,\n",
        "    secret_access_key,\n",
        "    path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com',\n",
        "    temp_file = 'temp.csv',\n",
        "    encoded_file = 'encoded_creds_pedro.csv'\n",
        "):\n",
        "    # setting paths and names\n",
        "    temp_path = os.path.join(path, temp_file)\n",
        "    encoded_path = os.path.join(path, encoded_file)\n",
        "\n",
        "    # generating key\n",
        "    key = Fernet.generate_key()\n",
        "    fernet = Fernet(key)\n",
        "\n",
        "    # creating dataframe and saving\n",
        "    df_creds = pd.DataFrame({'access_key': access_key,\n",
        "                            'secret_access_key': secret_access_key},\n",
        "                            index=[0])\n",
        "    df_creds.to_csv(temp_path)\n",
        "\n",
        "    # reading data\n",
        "    with open(temp_path, 'rb') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # encrypting data and saving\n",
        "    encrypted_data = fernet.encrypt(data)\n",
        "    with open(encoded_path, 'wb') as f:\n",
        "        f.write(encrypted_data)\n",
        "\n",
        "    delete_file(path, temp_file)\n",
        "\n",
        "    return key\n",
        "\n",
        "def get_aws_creds(\n",
        "    key,\n",
        "    path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com',\n",
        "    encoded_file = 'encoded_creds_pedro.csv',\n",
        "    decoded_file = 'decoded_creds_pedro.csv'\n",
        "):\n",
        "    # setting paths and names\n",
        "    encoded_path = os.path.join(path, encoded_file)\n",
        "    decoded_path = os.path.join(path, decoded_file)\n",
        "\n",
        "    # opening the encrypted file\n",
        "    with open(encoded_path, 'rb') as enc_file:\n",
        "        encrypted = enc_file.read()\n",
        "\n",
        "    # decrypting the file\n",
        "    fernet = Fernet(key)\n",
        "    decrypted = fernet.decrypt(encrypted)\n",
        "\n",
        "    # writing the decoded creds\n",
        "    with open(decoded_path, 'wb') as dec_file:\n",
        "        dec_file.write(decrypted)\n",
        "    df_dec = pd.read_csv(decoded_path)\n",
        "\n",
        "    delete_file(path, decoded_file)\n",
        "\n",
        "    return df_dec['access_key'].values[0], df_dec['secret_access_key'].values[0]\n",
        "\n",
        "# access_key, secret_access_key = get_aws_creds(\n",
        "#     key=,\n",
        "#     path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com',\n",
        "#     encoded_file = 'encoded_creds_pedro.csv'\n",
        "# )"
      ],
      "metadata": {
        "id": "vHBezFH6ligO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction"
      ],
      "metadata": {
        "id": "Bp0nrRJ9ltP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_csv_from_bucket('churn_preprocessed_18102022.csv')\n",
        "labels = ['contract_id', 'e_desligado']\n",
        "features = list(set(df.columns) - set(labels))\n",
        "df = df[labels + features]\n",
        "df"
      ],
      "metadata": {
        "id": "ifza8cRalteD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifiers"
      ],
      "metadata": {
        "id": "cNAXNUFzlzOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import lightgbm as lgb\n",
        "\n",
        "def get_classifiers(\n",
        "    RANDOM_STATE = 42\n",
        "):\n",
        "\n",
        "    classifiers = {\n",
        "        'model': [\n",
        "                    # 'BalancedBagging+RandomOverSampler',\n",
        "                    # 'Bagging+DecisionTree',\n",
        "                    # 'BaggingClassifier',\n",
        "                    # 'BalancedBagging+SMOTE',\n",
        "                    # 'BalancedBagging+roughly_balanced_bagging',\n",
        "\n",
        "                    'Balanced bag of histogram gradient boosting',\n",
        "\n",
        "                    # 'HistGradientBoostingClassifier',\n",
        "                    'Balanced RandomForest',\n",
        "                    # 'BalancedBagging+RandomUnderSample',\n",
        "                    'Undersampling RandomForest',\n",
        "                    'Undersampling LogisticRegression',\n",
        "                    # 'RandomForest+BalancedClassWeights',\n",
        "                    'Undersampling XGBoost',\n",
        "                    'Oversampling XGBoost',\n",
        "                    # 'XGBoost',\n",
        "                    'Bagging+KNN10',\n",
        "                    # 'Bagging LinearSVC',\n",
        "                    # 'LinearSVC'\n",
        "\n",
        "                    'LightGBM',\n",
        "                    'CatBoost'\n",
        "        ],\n",
        "        'classifier':[\n",
        "                    # BalancedBaggingClassifier(\n",
        "                    #                             sampler=RandomOverSampler(),\n",
        "                    #                             n_estimators=100),\n",
        "                    # BaggingClassifier(\n",
        "                    #                     base_estimator=DecisionTreeClassifier(),\n",
        "                    #                     random_state=RANDOM_STATE,\n",
        "                    #                     n_estimators=100),\n",
        "                    # BaggingClassifier(n_estimators=100),\n",
        "                    # BalancedBaggingClassifier(sampler=SMOTE(),\n",
        "                    #                             n_estimators=100),\n",
        "                    # BalancedBaggingClassifier(\n",
        "                    #     sampler=FunctionSampler(func=roughly_balanced_bagging, kw_args={\"replace\": True}),\n",
        "                    #     n_estimators=25\n",
        "                    # ),\n",
        "\n",
        "                    make_pipeline(\n",
        "                        BalancedBaggingClassifier(\n",
        "                            base_estimator=HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
        "                            n_estimators=10,\n",
        "                            random_state=RANDOM_STATE,\n",
        "                            n_jobs=2,\n",
        "                        ),\n",
        "                    ),\n",
        "\n",
        "                    # HistGradientBoostingClassifier(random_state=RANDOM_STATE,\n",
        "                    #                                 learning_rate=0.9),\n",
        "                    make_pipeline(\n",
        "                      BalancedRandomForestClassifier(random_state=RANDOM_STATE,\n",
        "                                                     n_jobs=2,\n",
        "                                                    n_estimators=50),\n",
        "                  ),\n",
        "                    # BalancedBaggingClassifier(sampler=RandomUnderSampler(),\n",
        "                    #                             n_estimators=25),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomUnderSampler(random_state=RANDOM_STATE),\n",
        "                        RandomForestClassifier(random_state=RANDOM_STATE,\n",
        "                                                n_jobs=2),\n",
        "                    ),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomUnderSampler(random_state=RANDOM_STATE),\n",
        "                        LogisticRegression(max_iter=1000),\n",
        "                    ),\n",
        "                    # RandomForestClassifier(random_state=RANDOM_STATE,\n",
        "                    #                         class_weight='balanced',\n",
        "                    #                         n_jobs=2,\n",
        "                    #                         n_estimators=50),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomUnderSampler(random_state=RANDOM_STATE),\n",
        "                        xgb.XGBClassifier(),\n",
        "                    ),\n",
        "                    make_pipeline_with_sampler(\n",
        "                        RandomOverSampler(random_state=RANDOM_STATE),\n",
        "                        xgb.XGBClassifier(),\n",
        "                    ),\n",
        "                    # xgb.XGBClassifier(scale_pos_weight=100),\n",
        "                    BaggingClassifier(\n",
        "                                        base_estimator=KNeighborsClassifier(n_neighbors=10),\n",
        "                                        random_state=RANDOM_STATE\n",
        "                                        ),\n",
        "                    # BaggingClassifier(\n",
        "                    #                     base_estimator=svm.SVC(kernel='linear',\n",
        "                    #                                             C=1,\n",
        "                    #                                             random_state=RANDOM_STATE,\n",
        "                    #                                             probability = True),\n",
        "                    #                     random_state=RANDOM_STATE\n",
        "                    #                     ),\n",
        "                    # svm.SVC(\n",
        "                    #     kernel='linear',\n",
        "                    #     random_state = RANDOM_STATE,\n",
        "                    #     class_weight='balanced',\n",
        "                    #     gamma='scale',\n",
        "                    #     C=1.1,\n",
        "                    #    probability=True\n",
        "                    # ),\n",
        "                    LGBMClassifier(random_state = RANDOM_STATE),\n",
        "                    CatBoostClassifier(random_state = RANDOM_STATE)\n",
        "        ],\n",
        "        'randsearch_params': [\n",
        "                        # # 'BalancedBagging+RandomOverSampler',\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # 'Bagging+DecisionTree'\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # BaggingClassifier\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # BalancedBagging+SMOTE\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # # BalancedBagging+roughly_balanced_bagging\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "\n",
        "                        # 'Balanced bag of histogram gradient boosting'\n",
        "                        {'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "\n",
        "                        # # HistGradientBoostingClassifier\n",
        "                        # {'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "                        # BalancedRandomForestClassifier\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # # 'BalancedBagging+RandomUnderSample'\n",
        "                        # {'n_estimators': [10, 25, 50, 100]},\n",
        "\n",
        "                        # 'Undersampling RandomForest'\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # 'Undersampling LogisticRegression'\n",
        "                        {'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "                        'tol': [1e-4, 1e-3],\n",
        "                        'C': [1.0, 0.9, 1.1]},\n",
        "\n",
        "                        # # 'RandomForest+BalancedClassWeights'\n",
        "                        # {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # 'Undersampling XGBoost'\n",
        "                        {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                        # 'Oversampling XGBoost'\n",
        "                        {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                        # # XGBoost\n",
        "                        # {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                        # 'Bagging+KNN10'\n",
        "                        {'weights': ['uniform', 'distance'],\n",
        "                        'n_neighbors': [3, 5, 8, 10, 15]},\n",
        "\n",
        "                        # # # Bagging+LinearSVC\n",
        "                        # # {'gamma': ['scale', 'auto'],\n",
        "                        # # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                        # # LinearSVC\n",
        "                        # {'gamma': ['scale', 'auto'],\n",
        "                        # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                        # LightGBM params\n",
        "                        {\n",
        "                            'max_depth':[-1, 50, 100, 200],\n",
        "                            'learning_rate': [0.001, 0.01, 0.1, 0.5, 0.9, 1.0, 1.2, 1.5],\n",
        "                            'n_estimators': [100, 150, 200, 250, 500],\n",
        "#                             'class_weight': [None, {0: 0.9, 1: 0.1}],\n",
        "                            'scale_pos_weight':[None, 1.0, 0.1, 0.05]\n",
        "                        },\n",
        "\n",
        "                        # CatBoost\n",
        "                        {\n",
        "                            'max_depth':[-1, 50, 100, 200],\n",
        "                            'learning_rate':[0.001, 0.01, 0.1, 0.5, 0.9, 1.0, 1.2, 1.5],\n",
        "                            'n_estimators': [100, 150, 200, 250, 500],\n",
        "                            'scale_pos_weight':[None, 1.0, 0.1, 0.05]\n",
        "                        },\n",
        "        ],\n",
        "        'gridsearch_params': [\n",
        "                        # # 'BalancedBagging+RandomOverSampler',\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'Bagging+DecisionTree'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'BaggingClassifier'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'BalancedBagging+SMOTE'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # # 'BalancedBagging+roughly_balanced_bagging'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "\n",
        "                        # 'Balanced bag of histogram gradient boosting'\n",
        "                        {'loss': ['auto']*5,\n",
        "                            'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "\n",
        "                        # # HistGradientBoostingClassifier\n",
        "                        # {'loss': ['auto']*5,\n",
        "                        #     'learning_rate': [0.0, 0.1, 0.5, 0.9, 1.0]},\n",
        "\n",
        "                        # BalancedRandomForestClassifier\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # # 'BalancedBagging+RandomUnderSample'\n",
        "                        # {'n_estimators': [10, 25, 50, 75, 100,\n",
        "                        #                     10, 25, 50, 75, 100],\n",
        "                        # 'warm_start': [False]*5 + [True]*5},\n",
        "\n",
        "                        # 'Undersampling RandomForest'\n",
        "                        {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                        # 'Undersampling LogisticRegression'\n",
        "                        {'penalty': ['l1', 'l2', 'elasticnet']*3,\n",
        "                        'C': [1.0, 1.0, 1.0,\n",
        "                                0.9, 0.9, 0.9,\n",
        "                                1.1, 1.1, 1.1]},\n",
        "\n",
        "                        #     # 'RandomForest+BalancedClassWeights'\n",
        "                        #     {'n_estimators': [10, 50, 100, 150, 200]},\n",
        "\n",
        "                            # 'Undersampling XGBoost'\n",
        "                            {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                            # 'Oversampling XGBoost'\n",
        "                            {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                            # # XGBoost\n",
        "                            # {'scale_pos_weight': [10, 50, 90, 99, 100]},\n",
        "\n",
        "                            # 'Bagging+KNN10'\n",
        "                            {'weights': ['uniform']*5 + ['distance']*5,\n",
        "                            'n_neighbors': [3, 5, 8, 10, 15]*2},\n",
        "\n",
        "                            # # # Bagging+LinearSVC\n",
        "                            # # {'gamma': ['scale']*4 + ['auto']*4,\n",
        "                            # # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                            # # LinearSVC\n",
        "                            # {'gamma': ['scale']*4 + ['auto']*4,\n",
        "                            # 'C': [1.0, 1.1, 1.5, 0.9]},\n",
        "\n",
        "                            # LGBM\n",
        "                            {},\n",
        "\n",
        "                            # CatBoost\n",
        "                            {}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "    # for k in list(classifiers.keys()):\n",
        "    #   print(f'key: {k}, len: {len(classifiers[k])}')\n",
        "\n",
        "    return pd.DataFrame(classifiers)\n",
        "\n",
        "df_classifiers = get_classifiers()\n",
        "df_classifiers"
      ],
      "metadata": {
        "id": "Z9UvTzWWlzyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função Teste de Modelos"
      ],
      "metadata": {
        "id": "Frr0j_Sll5g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode('utf-8')\n",
        "\n",
        "\n",
        "def filter_alphanum_str(x):\n",
        "    return remove_accents(re.sub(r'\\W+', '', x))\n",
        "\n",
        "def get_features_labels_subset(df,\n",
        "                               features,\n",
        "                               p = 0.8,\n",
        "                               target_label = 'e_desligado',\n",
        "                               adjust_columns = False\n",
        "                              ):\n",
        "    t = df.copy()\n",
        "\n",
        "    if adjust_columns:\n",
        "        t.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   t.columns))\n",
        "        t.columns = list(map(filter_alphanum_str, t.columns))\n",
        "\n",
        "        features = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   features))\n",
        "        features = list(map(filter_alphanum_str, features))\n",
        "\n",
        "\n",
        "    N = len(t)\n",
        "    N_train = round(N * p)\n",
        "    N_test = N - N_train\n",
        "\n",
        "    X_train, X_test = t[features].loc[:N_train], t[features].loc[N_train:]\n",
        "    y_train, y_test = t[target_label].loc[:N_train].values, t[target_label].loc[N_train:].values\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf,\n",
        "    model_name,\n",
        "    adjust_X_train_columns = False\n",
        "):\n",
        "    if adjust_X_train_columns:\n",
        "        X_train.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-',''),\n",
        "                                   X_train.columns))\n",
        "        X_test.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-',''),\n",
        "                                   X_test.columns))\n",
        "\n",
        "        X_train.columns = list(map(filter_alphanum_str, X_train.columns))\n",
        "        X_test.columns = list(map(filter_alphanum_str, X_test.columns))\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_pred_proba = clf.predict_proba(X_test)\n",
        "\n",
        "    report = pd.DataFrame(classification_report_imbalanced(y_test, y_pred, output_dict=True))\n",
        "\n",
        "    d = {\n",
        "        'metric': list(report[1].index[:5]),\n",
        "        'values': list(map(lambda x: round(x, 3), report[1].values[:5])),\n",
        "        'model': len(report[1].values[:5])*[model_name]\n",
        "    }\n",
        "    d['metric'].append('accuracy')\n",
        "    d['values'].append(round(accuracy_score(y_true = y_test,\n",
        "                                     y_pred = y_pred),\n",
        "                             3)\n",
        "                      )\n",
        "    d['model'].append(model_name)\n",
        "    report_churn = pd.DataFrame(d)\n",
        "\n",
        "    return report_churn"
      ],
      "metadata": {
        "id": "N5idaPZIl54w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_features_labels_balanced(\n",
        "    df,\n",
        "    features,\n",
        "    p = 0.8,\n",
        "    target_label = 'e_desligado',\n",
        "   adjust_columns = False\n",
        "):\n",
        "    t = df.copy()\n",
        "\n",
        "    # shuffling indexes\n",
        "    t = t.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    if adjust_columns:\n",
        "        t.columns = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   t.columns))\n",
        "        t.columns = list(map(filter_alphanum_str, t.columns))\n",
        "\n",
        "        features = list(map(\n",
        "            lambda x: x.replace('.0', '').replace('>', 'maior').replace('<','menor').replace('[','').replace(']','').replace('-','').replace(',','_').replace('+','mais'),\n",
        "                                   features))\n",
        "        features = list(map(filter_alphanum_str, features))\n",
        "\n",
        "    # train data qtd\n",
        "    N = len(t)\n",
        "    N_train = math.ceil(N * p)\n",
        "    N_test = N - N_train\n",
        "\n",
        "    # initial balancement\n",
        "    qtd_pos = len(t[t[target_label] == 1])\n",
        "    qtd_neg = len(t[t[target_label] == 0])\n",
        "    print('qtd_pos: {} ({:.3f}%), qtd_neg: {} ({:.3f}%)'.format(\n",
        "        qtd_pos, (qtd_pos*100/(qtd_pos+qtd_neg)),\n",
        "        qtd_neg, (qtd_neg*100/(qtd_pos+qtd_neg))\n",
        "    ))\n",
        "    # label proportion\n",
        "    N_train_pos = math.ceil(qtd_pos * p)\n",
        "    N_train_neg =  math.ceil(qtd_neg * p)\n",
        "    N_tot = N_train_pos + N_train_neg\n",
        "    print('N_train_pos: {} ({:.3f}%), N_train_neg: {} ({:.3f}%)'.format(\n",
        "        N_train_pos, (N_train_pos*100/N_tot),\n",
        "        N_train_neg, (N_train_neg*100/N_tot)\n",
        "    ))\n",
        "    # positive samples\n",
        "    pos_train_data_sample = t[t[target_label] == 1].reset_index(drop=True).loc[:N_train_pos]\n",
        "\n",
        "    X_train_pos = pos_train_data_sample[features]\n",
        "    y_train_pos = pos_train_data_sample[target_label].values\n",
        "\n",
        "    # negative samples\n",
        "    neg_train_data_sample = t[t[target_label] == 0].reset_index(drop=True).loc[:N_train_neg]\n",
        "\n",
        "    X_train_neg = neg_train_data_sample[features]\n",
        "    y_train_neg = neg_train_data_sample[target_label].values\n",
        "\n",
        "\n",
        "    # Training data\n",
        "    X_train = pd.concat([X_train_pos, X_train_neg],\n",
        "                       axis = 0)\n",
        "    y_train = list(y_train_pos) + list(y_train_neg)\n",
        "\n",
        "\n",
        "    # test data\n",
        "    test_data_sample = t[\n",
        "        (~t.index.isin(pos_train_data_sample.index)) &\n",
        "        (~t.index.isin(neg_train_data_sample.index))\n",
        "    ]\n",
        "    X_test = test_data_sample[features]\n",
        "    y_test = test_data_sample[target_label].values\n",
        "\n",
        "    return X_train, np.array(y_train), X_test, np.array(y_test)\n"
      ],
      "metadata": {
        "id": "Rww9OCDhmHO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balanceamento dos Dados\n",
        "\n"
      ],
      "metadata": {
        "id": "lGR9r8r4mIl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                           features,\n",
        "                                           p = 0.8,\n",
        "                                          adjust_columns = True\n",
        "                                   )\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "id": "Z428EyHomI9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste - Todas as Features"
      ],
      "metadata": {
        "id": "iLxmG90AmOgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(set(df.columns) - set(labels))\n",
        "\n",
        "feats_remove = [\n",
        "    'churn_não churnou',\n",
        "    'churn_churnou',\n",
        "    'tempo_churn_meses',\n",
        "    'prestador_mais_freq_nan',\n",
        "#     'tipo_contato_nps_Membro - Cancelado',\n",
        "#     'tipo_contato_nps_Contratante - Não membro'\n",
        "]\n",
        "features = list(set(features) - set(feats_remove))\n",
        "features = list(filter(lambda x: 'não preenchido' not in x and x[-4:] != '_na',\n",
        "                      features\n",
        "                      ))\n",
        "X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                           features,\n",
        "                                           p = 0.8,\n",
        "                                          adjust_columns = True\n",
        "                                   )\n",
        "\n",
        "report_dummy_clf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = DummyClassifier(),\n",
        "    model_name = 'Dummy Classifier'\n",
        ")\n",
        "report_log_reg = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = LogisticRegression(),\n",
        "    model_name = 'Logistic Reg'\n",
        ")\n",
        "report_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = RandomForestClassifier(random_state = 42),\n",
        "    model_name = 'Random Forest'\n",
        ")\n",
        "report_us_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling RandomForest']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp RandomForest'\n",
        ")\n",
        "report_brf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Balanced RandomForest']['classifier'].values[0],\n",
        "    model_name = 'Balanced RandomForest'\n",
        ")\n",
        "report_us_xb = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling XGBoost']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp XGBoost'\n",
        ")\n",
        "report_lgbm = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'LightGBM']['classifier'].values[0],\n",
        "    model_name = 'LightGBM'\n",
        ")\n",
        "report_catboost = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'CatBoost']['classifier'].values[0],\n",
        "    model_name = 'CatBoost'\n",
        ")\n",
        "\n",
        "reports = pd.concat([\n",
        "                        report_dummy_clf,\n",
        "                        report_log_reg,\n",
        "                        report_rf,\n",
        "                        report_us_rf,\n",
        "                        report_brf,\n",
        "                        report_us_xb,\n",
        "                        report_lgbm,\n",
        "                        report_catboost\n",
        "                    ], axis = 0)\n",
        "\n",
        "fig = px.line_polar(reports,\n",
        "                        r=\"values\",\n",
        "                        theta='metric',\n",
        "                       color = 'model',\n",
        "                        line_close=True,\n",
        "                        height = 400,\n",
        "                        width = 700\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features)),\n",
        "    title_x = 0.5,\n",
        "#         title_y = 0.6,\n",
        "    xaxis_title=\"variáveis\",\n",
        "    yaxis_title=\"minmax score\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=12,\n",
        "    ),\n",
        "    legend = dict(\n",
        "        title = 'Modelos iniciais'\n",
        "    )\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(reports,\n",
        "                     y=\"values\",\n",
        "                    x='metric',\n",
        "                    color = 'model',\n",
        "                    barmode = 'group',\n",
        "                    text='values',\n",
        "                    height = 400,\n",
        "                    width = 1100\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features)),\n",
        "    title_x = 0.5,\n",
        "    yaxis_title=\"Performance\",\n",
        "    xaxis_title=\"Métrica\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=16,\n",
        "    ),\n",
        ")\n",
        "# out_path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com/plots_churn'\n",
        "# titulo = 'remocao_dataleakage_19102022'\n",
        "# out_file = os.path.join(out_path,\n",
        "#                        'clf_results_{}.html'.format(\n",
        "#                             titulo\n",
        "#                             )\n",
        "#                        )\n",
        "# fig.write_html(out_file)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0H2GdUUFmN8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sem suspeitas de Data Leakage"
      ],
      "metadata": {
        "id": "xjwN-VycmgdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(filter(lambda x: 'membro' in x.lower() and 'cancelado' in x.lower(), features))"
      ],
      "metadata": {
        "id": "36samiVcmT47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features2 = list(set(df.columns) - set(labels))\n",
        "\n",
        "feats_remove = [\n",
        "    'churn_não churnou',\n",
        "    'churn_churnou',\n",
        "    'tempo_churn_meses',\n",
        "    'prestador_mais_freq_nan',\n",
        "    'tipo_contato_nps_Membro - Cancelado',\n",
        "    'tipo_contato_nps_Contratante - Não membro'\n",
        "]\n",
        "features2 = list(set(features2) - set(feats_remove))\n",
        "features2 = list(filter(lambda x: 'não preenchido' not in x and x[-4:] != '_na',\n",
        "                      features2\n",
        "                      ))\n",
        "# data_leakage_suspects = list(filter(lambda x: 'membro' in x.lower() and 'cancelado' in x.lower(),\n",
        "#                                    df.columns))\n",
        "data_leakage_suspects = list(filter(lambda x: 'cancel' in x.lower(), df.columns))\n",
        "\n",
        "features2 = list(set(features2) - set(data_leakage_suspects))\n",
        "\n",
        "list(filter(lambda x: 'membro' in x.lower() and 'cancelado' in x.lower(), features2))"
      ],
      "metadata": {
        "id": "GmND8VLImYFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_leakage_suspects"
      ],
      "metadata": {
        "id": "St0ep4lmmaJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = get_features_labels_balanced(df,\n",
        "                                           features2,\n",
        "                                           p = 0.8,\n",
        "                                          adjust_columns = True\n",
        "                                   )\n",
        "\n",
        "report_dummy_clf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = DummyClassifier(),\n",
        "    model_name = 'Dummy Classifier'\n",
        ")\n",
        "report_log_reg = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = LogisticRegression(),\n",
        "    model_name = 'Logistic Reg'\n",
        ")\n",
        "report_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = RandomForestClassifier(random_state = 42),\n",
        "    model_name = 'Random Forest'\n",
        ")\n",
        "report_us_rf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling RandomForest']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp RandomForest'\n",
        ")\n",
        "report_brf = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Balanced RandomForest']['classifier'].values[0],\n",
        "    model_name = 'Balanced RandomForest'\n",
        ")\n",
        "report_us_xb = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'Undersampling XGBoost']['classifier'].values[0],\n",
        "    model_name = 'UnderSamp XGBoost'\n",
        ")\n",
        "report_lgbm = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'LightGBM']['classifier'].values[0],\n",
        "    model_name = 'LightGBM'\n",
        ")\n",
        "report_catboost = test_model_performance(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    clf = df_classifiers[df_classifiers['model'] == 'CatBoost']['classifier'].values[0],\n",
        "    model_name = 'CatBoost'\n",
        ")\n",
        "\n",
        "reports2 = pd.concat([\n",
        "                        report_dummy_clf,\n",
        "                        report_log_reg,\n",
        "                        report_rf,\n",
        "                        report_us_rf,\n",
        "                        report_brf,\n",
        "                        report_us_xb,\n",
        "                        report_lgbm,\n",
        "                        report_catboost\n",
        "                    ], axis = 0)\n",
        "\n",
        "fig = px.line_polar(reports2,\n",
        "                        r=\"values\",\n",
        "                        theta='metric',\n",
        "                       color = 'model',\n",
        "                        line_close=True,\n",
        "                        height = 400,\n",
        "                        width = 700\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features2)),\n",
        "    title_x = 0.5,\n",
        "#         title_y = 0.6,\n",
        "    xaxis_title=\"variáveis\",\n",
        "    yaxis_title=\"minmax score\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=12,\n",
        "    ),\n",
        "    legend = dict(\n",
        "        title = 'Modelos iniciais'\n",
        "    )\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(reports,\n",
        "                     y=\"values\",\n",
        "                    x='metric',\n",
        "                    color = 'model',\n",
        "                    barmode = 'group',\n",
        "                    text='values',\n",
        "                    height = 400,\n",
        "                    width = 1100\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Performances - previsão de churn ({} features)'.format(len(features2)),\n",
        "    title_x = 0.5,\n",
        "    yaxis_title=\"Performance\",\n",
        "    xaxis_title=\"Métrica\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=16,\n",
        "    ),\n",
        ")\n",
        "# out_path = '/dbfs/FileStore/shared_uploads/pedro.bloss@samisaude.com/plots_churn'\n",
        "# titulo = 'remocao_dataleakage_19102022'\n",
        "# out_file = os.path.join(out_path,\n",
        "#                        'clf_results_{}.html'.format(\n",
        "#                             titulo\n",
        "#                             )\n",
        "#                        )\n",
        "# fig.write_html(out_file)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6CltaoSjmc-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparação"
      ],
      "metadata": {
        "id": "DJcsah8ZmjdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_performance(reports,\n",
        "                        teste):\n",
        "    d = {\n",
        "    'metric':[],\n",
        "    'value': [],\n",
        "    '%':[],\n",
        "    'test':[]\n",
        "\n",
        "    }\n",
        "    metrics = list(reports['metric'].unique())\n",
        "    for metric in metrics:\n",
        "        d['metric'].append(metric)\n",
        "        d['value'].append(reports[reports['metric']==metric].mean()[0])\n",
        "        d['%'].append('{:.2f}%'.format(reports[reports['metric']==metric].mean()[0]*100))\n",
        "        d['test'].append(teste)\n",
        "\n",
        "    return pd.DataFrame(d)\n",
        "get_mean_performance(reports, 'original')"
      ],
      "metadata": {
        "id": "Q15JmjyEmju9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = pd.concat([\n",
        "            get_mean_performance(reports, 'original'),\n",
        "            get_mean_performance(reports2, 'removendo variáveis suspeitas')\n",
        "        ],\n",
        "        axis=0).reset_index(drop=True)\n",
        "\n",
        "fig = px.bar(t,\n",
        "                     y=\"value\",\n",
        "                    x='metric',\n",
        "                    color = 'test',\n",
        "                    barmode = 'group',\n",
        "                    text='%',\n",
        "                    height = 400,\n",
        "                    width = 1200\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Analisando possibilidade de data leakage',\n",
        "    title_x = 0.5,\n",
        "    yaxis_title=\"Performance média\",\n",
        "    xaxis_title=\"Métrica\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=16,\n",
        "    ),\n",
        "    legend = dict(title = 'Teste')\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WSCwFoOdmoX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = list(reports['model'].unique())\n",
        "metrics = list(reports['metric'].unique())\n",
        "\n",
        "d_all = {\n",
        "    'model':[],\n",
        "    'metric_dif':[],\n",
        "    'value_dif':[],\n",
        "    'pct_dif':[]\n",
        "}\n",
        "\n",
        "for model in models:\n",
        "    t1 = reports[reports['model']==model].copy()\n",
        "    t1['teste'] = len(t1)*['original']\n",
        "\n",
        "    t2 = reports2[reports2['model']==model].copy()\n",
        "    t2['teste'] = len(t2)*['removendo variáveis suspeitas']\n",
        "\n",
        "    t = pd.concat([t1, t2], axis=0).reset_index(drop=True)\n",
        "    t['pct'] = list(map(lambda x: round(x*100, 3), t['values'].values))\n",
        "\n",
        "    for metric in metrics:\n",
        "        d_all['metric_dif'].append(metric)\n",
        "        d_all['value_dif'].append(\n",
        "            t1[t1['metric']==metric]['values'].values[0] - t2[t2['metric']==metric]['values'].values[0]\n",
        "        )\n",
        "        d_all['pct_dif'].append(\n",
        "            '{:.1f}%'.format((t1[t1['metric']==metric]['values'].values[0] - t2[t2['metric']==metric]['values'].values[0])*100)\n",
        "        )\n",
        "        d_all['model'].append(model)\n",
        "\n",
        "t_all = pd.DataFrame(d_all)\n",
        "\n",
        "fig = px.bar(t_all,\n",
        "                     y=\"value_dif\",\n",
        "                    x='metric_dif',\n",
        "                    color = 'model',\n",
        "                    barmode = 'group',\n",
        "                    text='pct_dif',\n",
        "                    height = 500,\n",
        "                    width = 1400\n",
        "                       )\n",
        "\n",
        "fig.update_layout(\n",
        "    title = 'Analisando possibilidade de data leakage (Diferenças em performance)',\n",
        "    title_x = 0.5,\n",
        "    yaxis_title=\"Diferença Performance\",\n",
        "    xaxis_title=\"Métrica (antes - depois)\",\n",
        "    font=dict(\n",
        "        family=\"Lato\",\n",
        "        size=12,\n",
        "    ),\n",
        "    legend = dict(title = 'Teste')\n",
        ")\n",
        "fig.update_traces(textfont_size=18, textangle=0, textposition=\"outside\", cliponaxis=False)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Rwbj14NfmyRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = list(reports['model'].unique())\n",
        "\n",
        "for model in models:\n",
        "    t1 = reports[reports['model']==model].copy()\n",
        "    t1['teste'] = len(t1)*['original']\n",
        "\n",
        "    t2 = reports2[reports2['model']==model].copy()\n",
        "    t2['teste'] = len(t2)*['removendo variáveis suspeitas']\n",
        "\n",
        "    t = pd.concat([t1, t2], axis=0).reset_index(drop=True)\n",
        "    t['pct'] = list(map(lambda x: round(x*100, 3), t['values'].values))\n",
        "\n",
        "    fig = px.bar(t,\n",
        "                         y=\"values\",\n",
        "                        x='metric',\n",
        "                        color = 'teste',\n",
        "                        barmode = 'group',\n",
        "                        text='pct',\n",
        "                        height = 300,\n",
        "                        width = 900\n",
        "                           )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title = 'Analisando possibilidade de data leakage ({})'.format(model),\n",
        "        title_x = 0.5,\n",
        "        yaxis_title=\"Performance\",\n",
        "        xaxis_title=\"Métrica\",\n",
        "        font=dict(\n",
        "            family=\"Lato\",\n",
        "            size=16,\n",
        "        ),\n",
        "        legend = dict(title = 'Teste')\n",
        "    )\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "Sz-BioJUmyMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNKsqol_myKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qxo1fuHWmyHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}